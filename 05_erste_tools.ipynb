{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agenten\n",
    "KI Agenten sind laut vielen Experten das nächste große Ding in Sachen KIs. Ganz unabhängig davon, ob das stimmt, können (autonome) KI Agenten hilfreich sein und es lohnt sich, sich damit auseinander zu setzen.\n",
    "Meiner Einschätzung nach erweitern KI Agenten das Spektrum an Werkzeugen, die man zum Lösen von Aufgaben benötigt. Diese ermöglichen auch Aufgaben zu löschen, die zuvor eigentlich nicht automatisierbar waren.\n",
    "\n",
    "Dennoch, bei aller Euphorie, muss man sagen, dass Stand jetzt autonome Agenten noch in den Kinderschuhen stecken und häufig Fehler begehen. Komplexe Aufgaben sind also noch in weiter Ferne.\n",
    "\n",
    "Heute schauen wir uns an, wie man Agenten erstellt und wozu sie gut sein könnten. Wir beginnen mit sehr einfachen Formen und steigern uns dann und setzen das Agentenframework LangGraph ein.\n",
    "\n",
    "\n",
    "## Was sind denn Agenten überhaupt?\n",
    "\n",
    "Die Definitionen, was ein Agent ist, gehen wie so oft stark auseinander. Wo sich die meisten jedoch einig sind, ist dass wenn man der KI Werkzeuge zur Verfügung stellt und die KI selbst entscheidet, wann sie diese Werkzeuge einsetzen möchte, die erste Stufe eines Agenten erreicht ist.\n",
    "Es gibt aber je nach Definition auch Agenten ohne Werkzeuge, also z.B. Agenten, die durch Prompting-Techniken erzeugt werden. \n",
    "\n",
    "Meist weist man einem Agenten eine Rolle zu, z.B. Organisator, Qualitätssicherer, Coder usw. Der Hintergrund ist der, dass Sprachmodelle häufig besser performen, wenn man sie in eine Rolle versetzt. In Agentensystemen, hat man also meist ein paar Agenten mit unterschiedlichen Rollen.\n",
    "In diesen Systemen tauschen die Agenten Daten aus, sei es dadurch, dass sie einen Prompt mit allen notwenigen Informationen bekommen und die Antwort wird z.B. einen anderen Agenten übergeben, oder z.B. durch einen gemeinsamen Speicher, auf den jeder Agent Zugriff hat.\n",
    "\n",
    "Gibt man einer KI die Möglichkeit Werkzeuge zu nutzen, läuft das häufig so ab, dass die KI die Ausführung anfragt und jemand oder etwas führt das Werkzeug aus und übergibt das Ergebnis wiederum an die KI, die dann damit arbeiten kann. D.h. die KI kann nicht einfach selbst etwas ausführen. Allerings bieten Agentensysteme die Möglichkeit an, dass diese Tool-Anfragen automatisch ausgeführt werden, ohne dass ein Mensch die Ausführung genehmigen muss oder gar selbst ausführen muss. Hier sprechen wir dann von *autonomen* Agenten. Häufig kombiniert man das aber mit _Human-In-The-Loop_ Ansätzen. D.h. bei kritischen Ausführungen wird ein Mensch erstmal um Erlaubnis gebeten. Auch wird das genutzt um z.B. zu entscheiden, wie etwas weitergehen soll, im Sinne von entweder Weg A, B oder C z.B. Oder um weitere Informationen abzufragen.\n",
    "\n",
    "Bei Systemen mit mehreren Agenten kann man für jeden Agent unterschiedliche Modelle verwenden. So kann man für jede Aufgabe das beste Modell auswählen. Während z.B. der Organisator-Agent ein Modell einsetzt, das gut logisch kombinieren kann, setzt der Coder ein Modell ein, das besonders gut Code produzieren kann.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Wie funktioniert das mit den Tools? Die KI, also das Sprachmodell, kann nicht selbst Funktionen oder ähnliches ausführen. Aber die KI kann darum bitten.\n",
    "\n",
    "Wenn man der KI also sagt, dass ihr Tools zur Verfügung stehen und man außerdem beschreibt, welche Argumente die Tools benötigen, dann kann die KI sich dafür entscheiden. In der Tat werden die Modelle darauf trainiert. Der Chat-Request an Chat-GPT (also die direkte OpenAI-Schnittstelle) enthält sogar einen besonderen Bereich außerhalb des Prompts für die Tools. Man übergibt dabei einen Namen des Tools und ganz wichtig eine Beschreibung, wofür das Tool gut sein soll. Die Beschreibung dient der KI dazu zu verstehen, was das Tool kann. Dann listet man die erwarteten Parameter auf. Auch hier mit Namen, Datentyp und Beschreibung. Und klar, die Beschreibung erklärt der KI wozu der Parameter gut ist.\n",
    "\n",
    "\n",
    "Das Ganze können wir mal kurz über einen simplen Prompt simulieren. Nehmen wir das allererste Beispiel, das wir programmiert haben und passen es an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.11/site-packages (0.1.9)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.11/site-packages (from langchain) (0.1.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain) (8.4.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.26.0 in ./.venv/lib/python3.11/site-packages (from langchain_openai) (1.35.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.11/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain) (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Es ist 15:03 Uhr.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "model = AzureChatOpenAI(openai_api_version=\"2024-05-01-preview\", azure_deployment=\"gpt-4o\", temperature=0.5)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Du bist ein hilfreicher Chatbot. Falls du nach der Uhrzeit gefragt wirst, dann kannst du ein Tool namens 'datetime' verwenden.\"\n",
    "                  \"Wenn du das Tool brauchst, dann antworte ausschließlich mit 'datetime'.\"),\n",
    "    HumanMessage(\"Wieviel Uhr ist es?\"),\n",
    "]\n",
    "\n",
    "response_text = model.invoke(messages)\n",
    "\n",
    "if response_text.content == \"datetime\":\n",
    "    import datetime\n",
    "    current_time = datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "    messages.append(AIMessage(f\"Antwort vom Tool ist: {current_time}. Beantworte jetzt die Frage des Nutzers\"))\n",
    "\n",
    "    response_text = model.invoke(messages)\n",
    "\n",
    "response_text.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das funktioniert zwar, aber es wäre doch sehr mühsam, wenn wir das jedes mal so machen müssten.\n",
    "\n",
    "Lasst uns also anschauen, was LangChain zu bieten hat. Hierfür werden wir allerdings noch nicht die Agenten-Funktionalität nutzen.\n",
    "Das Stichwort heißt _Tools_. Wir können also Tools, also Werkzeuge, anlegen bzw. anbieten und LangChain kümmert sich um den Rest. Es gibt wie immer zahlreiche fertige Tools, die man einsetzen kann.\n",
    "\n",
    "Erstmal müssen wir aber etwas installieren, nämlich DuckDuckGo-Search. Das wollen wir nämlich als Tool in unserer kleinen Anwendung nutzen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckduckgo-search in ./.venv/lib/python3.11/site-packages (6.1.9)\n",
      "Requirement already satisfied: click>=8.1.7 in ./.venv/lib/python3.11/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Requirement already satisfied: pyreqwest-impersonate>=0.4.9 in ./.venv/lib/python3.11/site-packages (from duckduckgo-search) (0.4.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstmal starten wir mit der Initialisierung. Wir importieren das Tool, das wir gleich nutzen möchten und legen uns davon eine Instanz an.\n",
    "Wir testen auch, ob das Tool auch so funktioniert, d.h. ohne es in den Workflow mit der KI einzubinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flieger der Extraklasse: Top 10 der schnellsten Vögel der Welt mit atemberaubenden Flugkünsten. Entdecke ihre Geschwindigkeit! Hast du dich jemals gefragt, wie schnell Vögel wirklich fliegen können? Vögel sind faszinierende Geschöpfe der Lüfte, und ihre Flugfähigkeiten sind einfach beeindruckend. In diesem Artikel werden wir gemeinsam in die erstaunliche Welt der Vogelgeschwindigkeiten eintauchen und einige der schnellsten Flieger der Tierwelt kennenlernen. Du wirst überrascht sein, wie unterschiedlich die ... Entdecken Sie den Sapsan, der am schnellsten fliegende vogel in der welt, und erfahren Sie mehr über seine atemberaubende Geschwindigkeit. Hinweis: Der Wanderfalke ist der schnellste Vogel der Welt. Im Sturzflug erreicht er Geschwindigkeiten von mehr als 300 Kilometer pro Stunde. „Entdecken Sie die außergewöhnliche Welt des Falken, eines der schnellsten Raubtiere der Welt. Dieser Artikel beschreibt die Hauptmerkmale dieser faszinierenden Art, ihren globalen Lebensraum und ihr überraschendes Verhalten. Tauchen Sie ein in die Evolutionsgeschichte des Falken und erfahren Sie, wie dieser majestätisch ist „Jäger überlebt in den wilden Landschaften, in denen er ...'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.run(\"Der schnellste Vogel der Welt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist schon mal cool, aber etwas mehr Informationen wären auch gut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[snippet: Er ist der größte lebende Vogel der Welt und kann eine Höhe von bis zu 2,5 Metern erreichen. Mit einem Gewicht von etwa 100 bis 150 Kilogramm ist der Strauß auch ein sehr schwerer Vogel. Der Strauß ist für seine beeindruckende Geschwindigkeit bekannt., title: Schnellster Flug der Welt: Die Top 10 der schnellsten Vögel, link: https://tierische-rekorde.com/schnellster-flug-der-welt-die-top-10-der-schnellsten-voegel-der-erde/], [snippet: Der Wanderfalke gilt als der schnellste Vogel der Welt und erreicht bei seinem Jagdflug eine Geschwindigkeit von 389 km/h. Falken werden seit Jahrhunderten in der Falknerei, der Kunst der Jagd mit Greifvögeln, eingesetzt. Die alten Ägypter verehrten den Falken als heilige Figur und er wurde in mehreren ihrer Hieroglyphen dargestellt., title: Der Falke: Eigenschaften, Lebensraum und Verhalten eines der ..., link: https://infoanimales.net/de/Falken/Wie-ist-ein-Falke?/], [snippet: Diese zierlichen Vögel sind für ihre wendigen Flugkünste bekannt und können atemberaubende Manöver in der Luft durchführen. Obwohl ihre Höchstgeschwindigkeiten nicht mit denen des Wanderfalken mithalten können, erreichen sie dennoch respektable 60 bis 70 Stundenkilometer. Doch die Geschwindigkeit ist nicht alles., title: Wie schnell fliegen Vögel? Alles über ihre Geschwindigkeiten, link: https://www.voegel-im-garten.de/wissenswertes/geschwindigkeiten-der-voegel/], [snippet: Der Vogel Strauss gilt als grösster Vogel der Welt. Er kann bis zu 150 Kilogramm schwer und 2,80 Meter hoch werden. Mit 72 km/h ist er auch der schnellste flugunfähige Vogel der Welt., title: Vogel-Rekorde - Die extremsten Vögel der Welt - SRF, link: https://www.srf.ch/radio-srf-musikwelle/vogel-rekorde-die-extremsten-voegel-der-welt]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "search.run(\"Der schnellste Vogel der Welt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bekommen wir auch Quellenangaben!\n",
    "\n",
    "Wie weiß jetzt die KI, wofür das Tool denn gut ist? Wir haben ja gelernt, dass jedes Tool eine Beschreibung braucht, damit die KI weiß wie und wofür es zu nutzen ist.\n",
    "Lass uns das mal ansehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around Duck Duck Go Search. Useful for when you need to answer questions about current events. Input should be a search query. Output is a JSON array of the query results'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query',\n",
       "  'description': 'search query to look up',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok bis jetzt haben wir das Tool praktisch selbst ausgeführt. Jetzt wollen wir es der KI an die virtuelle Hand geben. Wir werden später bei den Agenten sehen, dass es etwas eleganter geht, als das was gleich folgt.\n",
    "\n",
    "Jetzt ist es nämlich so, dass nicht jedes KI-Modell so gut in der Lage ist mit Tools umzugehen. OpenAIs ChatGPT ist z.B. extra dafür trainiert worden und man hat den Tools sogar einen extra Platz in der API gewidmet. Da kann man nämlich neben der Prompthistorie auch Tools mitgeben. Dort heißen sie allerdings _Functions_.\n",
    "Die _Functions_ sind eine Liste von Funktionen, wie der Name schon andeutet, wo man Name, Beschreibung und Parameter angibt. Vermutlich werden diese Angaben irgendwie automatisch in den System-Prompt der KI eingefügt, sodass die KI weiß welche Tools da sind.\n",
    "\n",
    "LangChain können wir dazu bringen, diese Funktionalität zu nutzen. Das schauen wir uns jetzt mal an..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'duckduckgo_results_json',\n",
       " 'description': 'A wrapper around Duck Duck Go Search. Useful for when you need to answer questions about current events. Input should be a search query. Output is a JSON array of the query results',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'query': {'description': 'search query to look up',\n",
       "    'type': 'string'}},\n",
       "  'required': ['query']}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "functions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, also offenbar gibt es eine Konvererfunktion, die aus den LangChain-Tools OpenAI-Functions macht.\n",
    "\n",
    "Jetzt können wir die _Functions_ auch bei der Konversation übergeben. Jetzt brauchen wir nur einen Prompt, der die KI dazu veranlasst dieses Tool zu nutzen und nicht einfach aus dem eigenen Trainingsdaten antwortet. Das geht am einfachsten, wenn man nach aktuellen Ereignissen fragt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Maß Bier Preis Münchener Oktoberfest 2024\"}', 'name': 'duckduckgo_results_json'}}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 103, 'total_tokens': 129}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_abc28019ad', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'function_call', 'logprobs': None, 'content_filter_results': {}}, id='run-83af4342-2339-42e9-956b-c68f7014623a-0', usage_metadata={'input_tokens': 103, 'output_tokens': 26, 'total_tokens': 129})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(\n",
    "    [HumanMessage(content=\"Wie viel kostet das Maß Bier auf dem Münchener Oktoberfest im Jahr 2024?\")], functions=functions\n",
    ")\n",
    "\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh ok! Was ist passiert? Nun, ChatGPT hat erkannt, dass wir suchen müssen und möchte gerne, dass wir das Tool _duckduckgo_results_json_ ausführen. Es gibt uns auch dafür die notwendigen Parameter mit. \n",
    "\n",
    "Jetzt müssten wir eigentlich loslegen und das ganze ausprogrammieren. D.h. wir müssten uns eine Routine bauen, die erkennt, welche Funktion ausgeführt werden soll. Dann müsste das Ergebnis als _ToolMessage_ an die KI übergeben werden, die dann daraus ihre finale Antwort baut.\n",
    "\n",
    "Wer möchte, kann das gerne als Übung machen. Aber das Ganze müssen wir doch nicht selbst erfinden, denn LangChain hat das ja schon eingebaut! Nämlich als Agent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
