{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertiefung\n",
    "\n",
    "Jetzt haben wir die allerersten Schritte getan und wagen uns an etwas komplexere Themen, bevor wir uns an die Tools für die KI herantasten. \n",
    "\n",
    "Zwei ganz wichtige Usecases für Anwendungen mit KI sind zum einen eine Chat-Funktion und das Bereitstellen von (eigenen) Daten. \n",
    "\n",
    "Zum Warm werden, schauen wir uns eine sehr simple Chat-Funktion an. Auch hierfür bietet LangChain eine Menge Hilfsmittel an. Grundsätzlich laufen Chats mit der KI so ab, dass man in jedem Request an die KI immer die gesamte Chat-Historie mitschickt. D.h. die KI (und die Software, die die KI bereitstellt) speichert nicht den Chatverlauf in der Regel. Das Speichern (im Arbeitsspeicher) der Historie übernimmt in LangChain-Jargon ein _Memory_. \n",
    "\n",
    "\n",
    "\n",
    "![LangChain Memory](https://python.langchain.com/v0.2/assets/images/message_history-4c13b8b9363beb4621d605bf6b5a34b4.png)\n",
    "\n",
    "Hinweis: _Runnable_ ist ein Interface, das die Grundlage eines Gliedes einer Kette bildet. D.h. man kann Runnables verketten.\n",
    "\n",
    "\n",
    "Hinweis: häufig nutzt man den Begriff _Kontext_. Damit sind alle Informationen gemeint, die bei der Kommunikation mit der KI mitgegeben werden (oder anderweitig dazugesteuert werden). Dazu gehören auch die Nachrichten aus dem gesamten Chatverlauf. Ein Thema vom Anfang des Chats, wird also auch am Ende als Kontext angesehen, auch wenn man das Thema zwischendrin komplett verändert hat. Daher mein Tipp: immer einen neuen Kontext anfangen, sobald man das Thema wechselt... das spart auch Kosten, denn der gesamte Chatverlauf wird jedes Mal neu verarbeitet und kostet somit immer mehr und mehr Token!\n",
    "\n",
    "\n",
    "So! Lange Rede, kurzer Sinn! Beispielcode sagt mehr als tausend Worte..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_openai langchain_community pypdf faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o\")\n",
    "model = AzureChatOpenAI(openai_api_version=\"2024-05-01-preview\", azure_deployment=\"gpt-4o\", temperature=0.5)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "system_template = \"Du bist ein sehr hilfreicher Chatbot, der alles dafür tut, um dem Nutzer zu helfen.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template), \n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok wir haben uns erstmal eine \"gewöhnliche\" Chain mit einem Prompt-Template, Modell und Parser angelegt. Bis hier eigentlich mit unserem ersten Gehversuch identisch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Deine Frage (gebe 'exit' zum Beenden ein): \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    antwort = chain_with_message_history.invoke({\"input\": user_input, \"chat_history\": chat_history}\n",
    "                                                , {\"configurable\": {\"session_id\": \"unused\"}})\n",
    "    print(f\"KI: {antwort}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist der eigentlich spannende Part. Wir legen uns eine Instanz einer ChatMessageHistory an. Diese kümmert sich darum die eingebenenen und die von der KI erzeugten Nachrichten zu verwalten.\n",
    "\n",
    "Damit legen wir uns ein _RunnableWithMessageHistory_ an. Das ist ein Helferlein aus LangChain, das unsere Chain mit der Speicherfunktionalität verknüpft. Das Ergebnis ist praktisch eine neue Chain, die wir aufrufen können.\n",
    "\n",
    "Das tun wir dann auch in einer Endlos-Schleife (die man mit der Eingabe von 'exit' verlassen kann). Erstmal fordern wir den/die Benutzer*in auf einen Prompt einzugeben. Dieser wird dann mit dem Schlüssel \"input\" übergeben.\n",
    "\n",
    "(Wir sehen hier auch, dass man eine _session_id_ übergeben kann. Damit könnte man unterschiedliche Konversationen implementeieren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt ein wenig \"Real Talk\" wie man es so schön nennt...\n",
    "\n",
    "Diese wundervolle Stück Code hätte ich nicht aus dem Ärmel schütteln können. Es ist auf einmal so unklar, warum wir auf einmal irgendein Runnable-Dings erzeugen müssen und das Muster mit der Verkettung verlassen wird. Sowas findet man leider ständig in diesem Framework. Dadurch ist man praktisch immer damit beschäftigt die Doku zu lesen, um solche Sachen umsetzen zu können. Zum Glück findet man für ziemlich alle Usecases schnell ein Stück Beispielcode. Aber gutes API-Design sieht m.E. anders aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier geht es zum nächsten Abschnitt: RAG: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ki-zeitalter/ai-agent-workshop/blob/main/03_rag.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
