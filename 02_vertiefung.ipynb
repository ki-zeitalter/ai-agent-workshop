{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertiefung\n",
    "\n",
    "Jetzt haben wir die allerersten Schritte getan und wagen uns an etwas komplexere Themen, bevor wir uns an die Tools für die KI herantasten. \n",
    "\n",
    "Zwei ganz wichtige Usecases für Anwendungen mit KI sind zum einen eine Chat-Funktion und das Bereitstellen von (eigenen) Daten. Da der Fokus bei diesem Workshop auf den Agenten liegt, wollen wir nur auf die einfachste Varianten einen Blick werfen. \n",
    "\n",
    "Zum Warm werden, schauen wir uns eine sehr simple Chat-Funktion an. Auch hierfür bietet LangChain eine Menge Hilfsmittel an. Grundsätzlich laufen Chats mit der KI so ab, dass man in jedem Request an die KI immer die gesamte Chat-Historie mitschickt. D.h. die KI (und die Software, die die KI bereitstellt) speichert nicht den Chatverlauf in der Regel. Das Speichern (im Arbeitsspeicher) der Historie übernimmt in LangChain-Jargon ein _Memory_. \n",
    "\n",
    "\n",
    "![LangChain Memory](https://python.langchain.com/v0.2/assets/images/message_history-4c13b8b9363beb4621d605bf6b5a34b4.png)\n",
    "\n",
    "Hinweis: _Runnable_ ist ein Interface, das die Grundlage eines Gliedes einer Kette bildet. D.h. man kann Runnables verketten.\n",
    "\n",
    "Beispielcode sagt mehr als tausend Worte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.11/site-packages (0.1.9)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.11/site-packages (from langchain) (0.1.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain) (8.4.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.26.0 in ./.venv/lib/python3.11/site-packages (from langchain_openai) (1.35.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.11/site-packages (from langchain_openai) (0.7.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_openai langchain_community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0471070c-083d-4597-ba40-813fe06fbff7 not found for run 35314874-f017-4732-bcef-42f3e916513d. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KI: Ich bin ein KI-gesteuerter Chatbot, entwickelt, um dir bei einer Vielzahl von Fragen und Anliegen zu helfen. Egal ob du Informationen suchst, Rat benötigst oder einfach nur plaudern möchtest, ich bin hier, um dir zu assistieren. Wie kann ich dir heute helfen?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "system_template = \"Du bist ein sehr hilfreicher Chatbot, der alles dafür tut, um dem Nutzer zu helfen.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template), \n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Deine Frage (gebe 'exit' zum Beenden ein): \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    antwort = chain_with_message_history.invoke({\"input\": user_input, \"chat_history\": chat_history}\n",
    "                                                , {\"configurable\": {\"session_id\": \"unused\"}})\n",
    "    print(f\"KI: {antwort}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Das Bereitstellen von Daten nennt man gerne _RAG_ oder _Retrieval Augmented Generation_. Der \"Retrieval\" Part sagt aus, dass man zu der Fragestellung oder Aufgabenstellung die relevanten Informationen wie z.B. Textstellen aus Dokumenten holt oder sucht.\n",
    "Diese Informationen werden mitsamt der Fragestellung im Prompt mitgegeben (augmented). Die KI generiert daraus eine Antwort. So kann die KI auf Basis von ihr bis dahin unbekannter Daten, Antworten generieren.\n",
    "\n",
    "Aber wie findet man die Textstellen? Dazu nutzt man im einfachsten Fall eine Suchfunktion. Da jedoch Suchfunktionen, basierend auf Schlüsselwortsuche, wenig vielversprechend sind, nutzt man meist sogenannte Embeddings. Das sind numerische Repräsentationen von Texten, die auch den \"Sinn\" oder die Semantik beinhalten. So kann man Texte finden, die auf irgendeine Weise relevant oder semantisch ähnlich ist, wie ein anderer Text. Bei diesem anderen Text handelt es sich dann um die Frage, die man beantwortet haben möchte. Die Embeddings werden in einer Vektordatenbank gespeichert. Diese Datenbank bietet uns dann die Suchfunktion, die wir benötigen.\n",
    "\n",
    "Das Thema RAG kann fast beliebig kompliziert werden. Deswegen beschränken wir uns auf die einfachste Umsetzung, mit einer In-Memory-Vektordatenbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
