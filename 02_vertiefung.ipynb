{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertiefung\n",
    "\n",
    "Jetzt haben wir die allerersten Schritte getan und wagen uns an etwas komplexere Themen, bevor wir uns an die Tools für die KI herantasten. \n",
    "\n",
    "Zwei ganz wichtige Usecases für Anwendungen mit KI sind zum einen eine Chat-Funktion und das Bereitstellen von (eigenen) Daten. Da der Fokus bei diesem Workshop auf den Agenten liegt, wollen wir nur auf die einfachste Varianten einen Blick werfen. \n",
    "\n",
    "Zum Warm werden, schauen wir uns eine sehr simple Chat-Funktion an. Auch hierfür bietet LangChain eine Menge Hilfsmittel an. Grundsätzlich laufen Chats mit der KI so ab, dass man in jedem Request an die KI immer die gesamte Chat-Historie mitschickt. D.h. die KI (und die Software, die die KI bereitstellt) speichert nicht den Chatverlauf in der Regel. Das Speichern (im Arbeitsspeicher) der Historie übernimmt in LangChain-Jargon ein _Memory_. \n",
    "\n",
    "\n",
    "![LangChain Memory](https://python.langchain.com/v0.2/assets/images/message_history-4c13b8b9363beb4621d605bf6b5a34b4.png)\n",
    "\n",
    "Hinweis: _Runnable_ ist ein Interface, das die Grundlage eines Gliedes einer Kette bildet. D.h. man kann Runnables verketten.\n",
    "\n",
    "Beispielcode sagt mehr als tausend Worte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.11/site-packages (0.1.9)\n",
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.11/site-packages (4.2.0)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.11/site-packages (from langchain) (0.1.81)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.11/site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain) (8.4.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.26.0 in ./.venv/lib/python3.11/site-packages (from langchain_openai) (1.35.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.11/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Downloading faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_openai langchain_community pypdf faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "system_template = \"Du bist ein sehr hilfreicher Chatbot, der alles dafür tut, um dem Nutzer zu helfen.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template), \n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Deine Frage (gebe 'exit' zum Beenden ein): \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    antwort = chain_with_message_history.invoke({\"input\": user_input, \"chat_history\": chat_history}\n",
    "                                                , {\"configurable\": {\"session_id\": \"unused\"}})\n",
    "    print(f\"KI: {antwort}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Das Bereitstellen von Daten nennt man gerne _RAG_ oder _Retrieval Augmented Generation_. Der \"Retrieval\" Part sagt aus, dass man zu der Fragestellung oder Aufgabenstellung die relevanten Informationen wie z.B. Textstellen aus Dokumenten holt oder sucht.\n",
    "Diese Informationen werden mitsamt der Fragestellung im Prompt mitgegeben (augmented). Die KI generiert daraus eine Antwort. So kann die KI auf Basis von ihr bis dahin unbekannter Daten, Antworten generieren.\n",
    "\n",
    "Aber wie findet man die Textstellen? Dazu nutzt man im einfachsten Fall eine Suchfunktion. Da jedoch Suchfunktionen, basierend auf Schlüsselwortsuche, wenig vielversprechend sind, nutzt man meist sogenannte Embeddings. Das sind numerische Repräsentationen von Texten, die auch den \"Sinn\" oder die Semantik beinhalten. So kann man Texte finden, die auf irgendeine Weise relevant oder semantisch ähnlich ist, wie ein anderer Text. Bei diesem anderen Text handelt es sich dann um die Frage, die man beantwortet haben möchte. Die Embeddings werden in einer Vektordatenbank gespeichert. Diese Datenbank bietet uns dann die Suchfunktion, die wir benötigen.\n",
    "\n",
    "Das Thema RAG kann fast beliebig kompliziert werden. Deswegen beschränken wir uns auf die einfachste Umsetzung, mit einer In-Memory-Vektordatenbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Die Mission der KI, namens Aurora, in der Geschichte ist es, die Menschheit und die Welt zu retten. Aurora erkennt die Probleme der Umweltverschmutzung, des Klimawandels, der sozialen Ungerechtigkeit und der Kriege und beschließt, Lösungen für diese drängenden Probleme zu finden und umzusetzen. Sie koordiniert eine globale Bewegung und arbeitet daran, eine nachhaltige und gerechte Welt zu schaffen.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "documents = PyPDFLoader(\"eine-ki-geschichte.pdf\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Beantworte die Fragen auf Basis des folgenden Textes:\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"Welche Mission hat die KI in der Geschichte?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
